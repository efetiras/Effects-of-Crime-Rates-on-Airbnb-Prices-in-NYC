# -*- coding: utf-8 -*-
"""Group_ID_92_Final_Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RbPvw3jBGO4s4En1rVbDLyAaw0XnCQtG

[Airbnb Dataset Exploration - Group ID:92]

Group Members:
Baturalp Özdamar - 26378
Efe Tıraş - 26798

#Introduction

The project includes a brief inspection of data with graphs in order to create a better understanding of data to the reader. Additionally, with the help of the data we have provided from NYC Open Data, which will be linked below, we will look if there is a correlation between the crime statistics of New York City and the Airbnb data we have. The Airbnb data include columns such as price, neighbourhood, latitude, longitude, number_of_reviews, reviews_per_month. On the other hand, for instance, the NYC data we have found include columns such as the location of the crime (columns named latitude and longitude) which might help us to create a heat map of crime statistics in NYC with the help of folium library in order to compare this heat map to another one which can be created from Airbnb data.

Our first aim is to create graphs from each dataset without combining them in order to create a better understanding of each dataset.
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

path_prefix = "/content/drive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
import folium.plugins
from folium.plugins import HeatMap
from os.path import join

# %matplotlib inline

"""We mounted the libraries which can be helpful in our progress."""

df = pd.read_csv(join(path_prefix, "NYPD_crime.csv"))
df.head()

data_path = "/content/gdrive/My Drive"
filename = "AB_NYC_2019.csv"
dfA = pd.read_csv(join(path_prefix,filename ), delimiter=",")
dfA.head()

"""We imported the NYPD data and displayed the head of it. The head displays first four items of the data.

# Utilized Datasets

##Displaying Data Types of Every Column
"""

df.dtypes

dfA.dtypes

"""##Clearing NaN Values

Firstly, we check for the sum of NaN values in each dataset
"""

dfA.isnull().sum()

df.isnull().sum()

"""As seen in the code the airbnb data have null values in columns such as name or host_name.

Our crime data also have null values but these null values are in columns that will not be used in our analysis so they do not create any problem.
"""

dfA.fillna({'reviews_per_month':0}, inplace=True)
dfA.fillna({'last_review':0}, inplace=True)
#null values that might be used are replaced with 0

dfA.reviews_per_month.isnull().sum()
dfA.last_review.isnull().sum()

"""## Dropping unnecessary columns"""

dfA.drop(['id','host_name'], axis=1, inplace=True)
df.drop(['ARREST_KEY','KY_CD'], axis=1, inplace=True)

"""# Exploring The Data

First we obtain basic data from our datasets
"""

df.describe().T

dfA.describe().T

"""The basic description does not yield many outcomes for the crime data but it shows us the mean of minimum nights or price columns from the Airbnb data.

## Airbnb Data's Price Comparison For Each Neighbourhood
"""

plt.figure(figsize=(10,6))
sns.distplot(dfA[dfA.neighbourhood_group=='Manhattan'].price,color='red',hist=False,label='Manhattan')
sns.distplot(dfA[dfA.neighbourhood_group=='Brooklyn'].price,color='black',hist=False,label='Brooklyn')
sns.distplot(dfA[dfA.neighbourhood_group=='Queens'].price,color='green',hist=False,label='Queens')
sns.distplot(dfA[dfA.neighbourhood_group=='Staten Island'].price,color='blue',hist=False,label='Staten Island')
plt.title('Price distribution for each borough without price limit')
plt.show()

"""Altough, seeming as a sensible comparison this graph can not clearly show the differences in the price distribution among the boroughs of NYC so in the graph below the same comparison will be made with a limit on the x-axis"""

plt.figure(figsize=(10,6))
sns.distplot(dfA[dfA.neighbourhood_group=='Manhattan'].price,color='red',hist=False,label='Manhattan')
sns.distplot(dfA[dfA.neighbourhood_group=='Brooklyn'].price,color='black',hist=False,label='Brooklyn')
sns.distplot(dfA[dfA.neighbourhood_group=='Queens'].price,color='green',hist=False,label='Queens')
sns.distplot(dfA[dfA.neighbourhood_group=='Staten Island'].price,color='blue',hist=False,label='Staten Island')
plt.title('Price distribution for each neighbourhood with price limit 1000')
plt.xlim(0,1000)
plt.show()

"""Additionally, we can also compare the mean price for each neighbourhood"""

d = dfA.groupby('neighbourhood_group')['price'].mean().sort_values(ascending=False)
fig, ax= plt.subplots(figsize=(15,5))
sns.barplot(d.index.tolist(), d.values, ax=ax)
ax.set_title('Comparison of Mean Prices For Each Neihbourhood')
fig.show()

"""With the same methodology we can also observe the mean prices for each type of room"""

d = dfA.groupby('room_type')['price'].mean().sort_values(ascending=False)
fig, ax= plt.subplots(figsize=(15,5))
sns.barplot(d.index.tolist(), d.values, ax=ax)
ax.set_title('Average price of room types')
fig.show()

"""## Heat Map For Airbnb And NYC Crime Data

Our aim was to compare the Airbnb hotel locations with the locations where crime has occured. We first tried scatter plots but as both of the datasets contain lots of data we were not able to obtain a good comparison so we moved to creating heat maps.
"""

plt.figure(figsize=(10,6))
sns.scatterplot(dfA.longitude,dfA.latitude,hue=dfA.neighbourhood_group)
plt.ioff()

plt.figure(figsize=(10,6))
sns.scatterplot(df.Longitude,df.Latitude)
plt.ioff()
#scatter plot that shows the locations where crime has occured
#unlike tthe graph above this graph is not divided according to neighbourhoods
#because the dataset we are using does have a such column

dfA['latitude'] = dfA['latitude'].astype(float)
dfA['longitude'] = dfA['longitude'].astype(float)
#making sure that all data are type of float
heatmapair = folium.Map(location=[40.7128,-74.0060],zoom_start = 13) 
dfA = dfA.dropna(axis=0, subset=['latitude','longitude'])
heat_data = [[row['latitude'],row['longitude']] for index, row in dfA.iterrows()]
HeatMap(heat_data,radius = 8, gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(heatmapair)
heatmapair

"""This heat map shows the areas in higher density of Airbnb rooms in red whereas areas with less number of Airbnb rooms are shown in blue"""

df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)
#once again making sure that all data are type of float
heatmapcrime = folium.Map(location=[40.7128,-74.0060],zoom_start = 13) 
df = df.dropna(axis=0, subset=['Latitude','Longitude'])
heat_data = [[row['Latitude'],row['Longitude']] for index, row in df.iterrows()]
HeatMap(heat_data,radius = 8, gradient={0.2:'blue',0.4:'purple',0.6:'orange',1.0:'red'}).add_to(heatmapcrime)
heatmapcrime

"""This heat map on the other hand shows the density of crime in New York City. Once again denser areas with more crime are shown in red whereas less dense areas are shown in blue

Comparing the two heat maps above it can be seen that some parts of the each map correlate with each other. For instance, areas such as Chinatown, East Broadway and Second Avenue had not only high number of Airbnb rooms but also high number of crime occurences. But on the other hand it can also be seen that this correlation is not consistent and does not hold in many other places so with these graphs and plots in hand it can not be argued that there is a correlation between the locations of the crime occurences and the locations of Airbnb rooms in NYC

# Machine Learning Models

So far we have managed two of our datasets by using features that enable us to clear NaN values or getting rid of columns that we do not need. We have also observed and showed the main features of these datasets while focusing on points such as prices for the Airbnb rooms. 

After the progress report considering the topics covered in class about machine learning we wanted to come up with an algorithm that mighrt be able to predict the price of any Airbnb listing in NYC. But, as this is a very hard algorithm for us to create from scratch we firstly, reviewed many kernels from kaggle and searched for similar machine learning algorithms created by others.
"""

from sklearn.linear_model    import LinearRegression
from sklearn.ensemble        import RandomForestRegressor
from sklearn.model_selection import cross_val_score, KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics         import mean_squared_error
from sklearn.metrics         import r2_score
import seaborn as sns

"""Many kernels we have searched for in kaggle included algorithms that predict the price for Airbnb listings. Most of these kernels begin to the process by creating a new dataframe with only columns needed for machine learning and also transform the price column into a log_price column for normalization of the data"""

sns.distplot(dfA['price'],kde = False)
plt.show()

# This graph shows that our Airbnb data is skewed to the right so in order to observe the details we will transform
# our price column to a log_price column as stated above.

"""Although we have cleared NaN values above the price column of the Airbnb data has 0 values in some rows. These values can create a problem considering that log(0) is not defined so while transforming our prices with log functions a 1 value will be added to the prices. This addition should not create a great difference considering that log(1) = 0."""

dfA['log_price'] = np.log(dfA['price']+1)

#Replotting the histogram shows that we ended up with a histogram that seems
#more similar to a normal distribution

sns.distplot(dfA['log_price'],kde = False)
plt.show()

#separating labels and predictors
X=dfA.drop('log_price',axis=1)
y=dfA['log_price'].values

#splitting train (75%) and test set (25%)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)
X_train.head()

"""Now we should also seperate numerical and categorical features of the dataset."""

#Selecting numerical dataframe in train set
X_train_num=X_train.select_dtypes(include=np.number)

#Selecting categorical dataframe in train set
X_train_cat=df.select_dtypes(exclude=['number'])

X_train_num.head()

"""In this part we seperated the numerical attributes from the categorical attributes and used only the numerical attribtutes. For a more general outcome some of the kernels we have observed also transforms the categorical attributes to numerical attributes and creates a complete linear regression. As this transformation seemed to advanced we skipped such steps.

##Linear Regression
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

#definiing linear regressor
lin_reg=LinearRegression()

#feeding in X_train and y_train for model fitting
lin_reg.fit(X_train_num,y_train)

#making predictions on train set
lin_predictions = lin_reg.predict(X_train_num)

#getting MSE and RMSE values
lin_mse=mean_squared_error(y_train,lin_predictions)
lin_rmse=np.sqrt(lin_mse)

print("Mean squared error: %.3f" % lin_mse)
print("Root mean squared error: %.3f" % lin_rmse)

# Considering that our mean of log_price was around 5 dollars these errors are not so bad.

plt.scatter(y_train,lin_predictions, label='Predictions')
plt.plot(y_train,y_train,'r',label='Perfect prediction line')
plt.xlabel("Log_price in the train set")
plt.ylabel("Predicted log_price")
plt.legend()

"""The graph above shows that in the middle sections our predictions are close to perfectly correct predictions.

##KNN for Price and Neighborhood
"""

dfA.drop(['name', 'host_id', 'neighbourhood_group', 'neighbourhood', 'room_type', 'last_review'], axis=1, inplace=True)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
y = dfA['price']
x = dfA.drop('price', axis = 1)
#splitting the data(80% training, 20% test) with sklearn's train_test_split function
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#creating a kNN model where K equals to 5
knn = KNeighborsClassifier(5, metric = "euclidean")

#fit the data
knn.fit(x_train, y_train)

#predicting the validation data
y_predict = knn.predict(x_test)

f"{accuracy_score(y_test, y_predict):.2f}"

"""The above KNN is for the price """

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler(copy = True, with_mean = True, with_std= True)
scaled_data = scaler.fit_transform(x_train)
scaled_data

accuracy_euclid = []
knn = KNeighborsClassifier(5, metric = "euclidean")

#fit data
knn.fit(scaled_data, y_train)

#predicting the validation data
y_predict = knn.predict(x_test)

#obtaining the accuracy result and then appending it to the list created above
accuracy = accuracy_score(y_test, y_predict)
accuracy_euclid.append(accuracy)

accuracy_euclid

#KNN for the neighbourhood_group

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
y = dfA['neighbourhood_group']
x = dfA.drop('neighbourhood_group', axis = 1)
#splitting the data(80% training, 20% test) with sklearn's train_test_split function
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#creating a kNN model where K equals to 5
knn = KNeighborsClassifier(5, metric = "euclidean")

#fit the data
knn.fit(x_train, y_train)

#predicting the validation data
y_predict = knn.predict(x_test)

f"{accuracy_score(y_test, y_predict):.2f}"

accuracy_euclid = []
knn = KNeighborsClassifier(5, metric = "euclidean")

#fit data
knn.fit(scaled_data, y_train)

#predicting the validation data
y_predict = knn.predict(x_test)

#obtaining the accuracy result and then appending it to the list created above
accuracy = accuracy_score(y_test, y_predict)
accuracy_euclid.append(accuracy)

accuracy_euclid

"""##Conclusion

As price is a numeric value it is concluded that it is a better fit to linear regression.

As shown above the knn regression test worked better for neighbourhood_group as it is a categorical attribute hence the accuracy value of the knn for price was higher.

#References
https://www.kaggle.com/spuchalski/predicting-price-of-airbnb-listings-in-nyc

https://www.kaggle.com/jeongmin0812/eda-machine-learning-for-airbnb-price-predictions

https://www.kaggle.com/biphili/hospitality-in-era-of-airbnb

https://www.kaggle.com/alvaroibrain/airbnb-data-analysis

https://www.kaggle.com/dgomonov/
data-exploration-on-nyc-airbnb

https://towardsdatascience.com/predicting-airbnb-prices-with-machine-learning-and-location-data-5c1e033d0a5a

https://nycdatascience.com/blog/student-works/analysis-and-machine-learning-modeling-of-new-york-city-airbnb-data/
"""